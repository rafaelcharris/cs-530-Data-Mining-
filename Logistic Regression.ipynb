{"cells":[{"cell_type":"markdown","metadata":{"id":"pXfFxXE0Pu7s"},"source":["# Logistic Regression\n","\n","\n","Run analysis using Logistic Regressions and at least 1 other method from class.\n","\n","Must include:\n","\n","  - Hyperparameter selection for the logistic regression C parameter\n","\n","  - Try fitting with l2 (ridge), l1 (lasso), and elastic regularization.\n","\n","  - Comparison of the methods you used (AUC, Matthew's Correlation Coeff, etc).\n","\n","You'll need to preprocess the data for the logistic regression by using one-hot encoding (be sure to have a baseline by leaving out a column).\n","\n","To compare models, first do a train-test split.  Set the test data aside.  Run k-fold cross validation to determine the hyperparameters and evaluate out-of-sample metrics.  Determine which model is projected to do best out of sample based on the k-fold cross validation.  Then, run the models with the selected hyperparameters on your original test dataset.  How well do your metrics hold up on the test set?  Are the model rankings preserved?"]},{"cell_type":"markdown","metadata":{"id":"IFPPtY5SQqOg"},"source":["## 0. Boilerplate"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2486,"status":"ok","timestamp":1646638982422,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"KgGDI16MPrfe"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn import metrics\n","from sklearn.naive_bayes import CategoricalNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn import preprocessing\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import make_column_transformer\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import cross_validate\n","\n","df = pd.read_csv('adult.csv', names=['age',\n","              'workclass',\n","              'fnlwgt',\n","              'education',\n","              'education-num',\n","              'marital-status',\n","              'occupation',\n","              'relationship', 'race',\n","              'sex', \n","              'capital-gain',\n","              'capital-loss',\n","              'hours-per',\n","              'native-country', 'y'], index_col = False).drop(['education'], axis = 1)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1646638982424,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"dwLNFIyZ0Ual","outputId":"67555022-e2a5-4014-ec55-f0801721b43d"},"outputs":[{"data":{"text/plain":["age               0\n","workclass         0\n","fnlwgt            0\n","education-num     0\n","marital-status    0\n","occupation        0\n","relationship      0\n","race              0\n","sex               0\n","capital-gain      0\n","capital-loss      0\n","hours-per         0\n","native-country    0\n","y                 0\n","dtype: int64"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df.isna().sum() # are there any null values ? -\u003e no "]},{"cell_type":"markdown","metadata":{"id":"4zoMVWRBosNS"},"source":["## 1. One-hot-encoding"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":209,"status":"ok","timestamp":1646638982618,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"hfZOmGa9HdFf"},"outputs":[],"source":["test = pd.read_csv('adult.test', names=['age',\n","              'workclass',\n","              'fnlwgt',\n","              'education',\n","              'education-num',\n","              'marital-status',\n","              'occupation',\n","              'relationship', 'race',\n","              'sex',\n","              'capital-gain',\n","              'capital-loss',\n","              'hours-per',\n","              'native-country', 'y'], index_col = False, skiprows = 1).drop(['education'], axis = 1)\n","\n","test['y'] = test['y'].apply(lambda x: x.replace('.', '').replace(' ', ''))\n","df['y'] = df['y'].apply(lambda x: x.replace('.', '').replace(' ', ''))\n","\n","X_train = df.drop(['y'], axis = 1)\n","y_train = df['y']\n","\n","X_test = test.drop(['y'], axis =1)\n","y_test = test['y']\n","\n","col_trans = make_column_transformer((OneHotEncoder(handle_unknown = 'ignore', # This argument handles categories that are missing in the train that appear in the test\n","                                                   drop = 'first'), # This drops the first column \n","                                    list(X_train.select_dtypes(include = 'O').columns)),\n","                                    remainder = 'passthrough'\n","                                     )"]},{"cell_type":"markdown","metadata":{"id":"gkjQJLwuHDMo"},"source":["I create a pipeline using `make_column_transformer` so the original data is not modified but only inside the `make_pipeline` function. \n","Then when I want to predict or fit the model, instead of using the instance of the model as usual, I have to use the instance of the pipeline. This is very benefitial because when I am using new data, I don't have to preprocess it before using the models function, but the data is processes inside the pipeline by using `pipe.predict(test)` instead of `clf.precict(test)`.\n","The `col_trans` object above does the OneHotEncoder for all the categorical variables, drops the first column of each, and handles unkown categories,i.e., categories that are on the test but not on the train data. Because I need the same transformations for all the regalarized regressions, I can pass it to the `make_pipeline` instance of each type of regression.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f6AXQCMuP2U1"},"source":["# 2. Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"oLBmzf7SCCVa"},"source":["## 2.1 Without Regularization\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5388,"status":"ok","timestamp":1646638988001,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"tbippMXbACWU","outputId":"65b698b3-a35b-49d9-fe95-abc771b7fd93"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:174: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n","  UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:174: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n","  UserWarning,\n"]}],"source":["clf_cv = LogisticRegression(random_state = 0, solver='liblinear')\n","\n","pipecv = make_pipeline(col_trans, clf_cv)\n","pipecv.fit(X_train, y_train)\n","preds = pipecv.predict(X_test)\n","\n","lr_cv = cross_validate(pipecv, X_train, y_train, scoring = ['roc_auc', 'accuracy'])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161,"status":"ok","timestamp":1646639129397,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"Y1XfFh3aVvv3","outputId":"515359a1-38d6-4b46-cccb-6d5f7212e076"},"outputs":[{"data":{"text/plain":["0.5888904636896759"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["lr_cv['test_roc_auc'].mean()"]},{"cell_type":"markdown","metadata":{"id":"dLSzUTSRWwZ-"},"source":["I am getting the same value of performance, no matter what value of C I pick. If the purpose of C is not to help increasing the out of sample performance what is the point."]},{"cell_type":"markdown","metadata":{"id":"u8rOJPVIXJ57"},"source":["## 2.1. Ridge\n","\n","For the regularized models I use the `LogisticRegressionCV` function because it finds the optimal hyperparameters with their built in cross-validation.\n","To indicate that this is a ridge regression, I use penalty `l2`."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46626,"status":"ok","timestamp":1646639048855,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"G-cx0UwqbaQM","outputId":"282beb59-dcfb-4822-8455-67fdde705aca"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:174: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n","  UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:174: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n","  UserWarning,\n"]},{"data":{"text/plain":["{'fit_time': array([ 8.39864016, 10.56440353,  7.59398127,  4.91445827,  4.66014934]),\n"," 'score_time': array([0.18503141, 0.26606941, 0.07986999, 0.09428239, 0.08082271]),\n"," 'test_accuracy': array([0.79671426, 0.79407248, 0.80006143, 0.79591523, 0.7997543 ]),\n"," 'test_roc_auc': array([0.58726249, 0.60428947, 0.58500873, 0.57615923, 0.59119408])}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["clf_ridge = LogisticRegressionCV(solver='liblinear', penalty = 'l2')\n","\n","pipe_ridge = make_pipeline(col_trans, clf_ridge)\n","pipe_ridge.fit(X_train, y_train)\n","preds_ridge = pipe_ridge.predict(X_test)\n","\n","ridge_cv = cross_validate(pipe_ridge, X_train, y_train, scoring = ['roc_auc', 'accuracy'])\n","ridge_cv"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1646639068922,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"JULtIibwVUvQ","outputId":"d4e33d68-6a58-4aab-89a2-d0ddc7a7c03e"},"outputs":[{"data":{"text/plain":["0.5887828003808759"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["ridge_cv['test_roc_auc'].mean()"]},{"cell_type":"markdown","metadata":{"id":"KhxkwuI3XJyp"},"source":["\n","## 2.2. Lasso\n","\n","To indicate that the regression is a lasso regression, I use the penalty `l1`."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90339,"status":"ok","timestamp":1646639229805,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"04nE32O6bgj-","outputId":"8afe59ba-327e-4e26-bb66-a30f4c326142"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:174: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n","  UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:174: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n","  UserWarning,\n"]}],"source":["clf_lasso = LogisticRegressionCV(solver='liblinear', penalty = 'l1')\n","\n","pipe_lasso = make_pipeline(col_trans, clf_lasso)\n","pipe_lasso.fit(X_train, y_train)\n","preds_lasso = pipe_lasso.predict(X_test)\n","\n","lasso_cv = cross_validate(pipe_lasso, X_train, y_train, scoring = ['roc_auc', 'accuracy'])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1646639253880,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"Ft9FBdQ7VlRU","outputId":"d31510bb-0309-41db-bfb1-0874cdac9b6d"},"outputs":[{"data":{"text/plain":["0.9059059899616223"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["lasso_cv['test_roc_auc'].mean()"]},{"cell_type":"markdown","metadata":{"id":"M4RyLx0oXJro"},"source":["## 2.3 Elastic Net\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TXbSjCIlbh9C"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-15-ed3a53a33caa\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipe_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0mpipe_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpreds_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   2180\u001b[0m                 \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m             )\n\u001b[0;32m-\u003e 2182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_encoded_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml1_ratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml1_ratios_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--\u003e 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--\u003e 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_log_reg_scoring_path\u001b[0;34m(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1079\u001b[0;31m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m     )\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    876\u001b[0m                 \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0mwarm_start_sag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 878\u001b[0;31m                 \u001b[0mis_saga\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"saga\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             )\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mintercept_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mis_saga\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 348\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["clf_en = LogisticRegressionCV(random_state = 0, solver='saga', penalty = 'elasticnet', l1_ratios = [0, 0.2, 0.4, 0.6 , 0.8, 1],\\\n","                              max_iter = 8000)\n","\n","pipe_en = make_pipeline(col_trans, clf_en)\n","pipe_en.fit(X_train, y_train)\n","preds_en = pipe_en.predict(X_test)\n","\n","elastic_cv = cross_validate(pipe_en, X_train, y_train, scoring = ['roc_auc', 'accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdQbTzL0Vog9"},"outputs":[],"source":["elastic_cv['test_roc_auc'].mean()"]},{"cell_type":"markdown","metadata":{"id":"VlHRR8C1Qln-"},"source":["## 3. Naive Bayes\n","\n","As an alternative to logistic regression, I use a Naive Bayes Classifier.\n","I need to convert all the variables into integers so the classifier works properly. This is what `le = preprocessing.LabelEncoder()` does.\n","I also have to bin continuous variables, with I do with the `pd.cut()` function in pandas."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":221,"status":"ok","timestamp":1646639264462,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"vRe6ACjPDs2m"},"outputs":[],"source":["le = preprocessing.LabelEncoder()\n","\n","def bining(df):\n","  '''\n","  Functions that bins all the numerical variables and bins them\n","  '''\n","  for col in df.select_dtypes(exclude = ['O']).columns:\n","    df[col] = pd.cut(df[col], bins = 10)\n","  return df\n","  \n","X_train['type'] = 1\n","X_test['type'] = 0\n","\n","X_full = pd.concat([X_train, X_test])\n","\n","min_categories=X_full.nunique()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1346,"status":"ok","timestamp":1646639267561,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"ZJI1tw-r75GC"},"outputs":[],"source":["# Create bins\n","X_full = bining(X_full)\n","\n","X_NB = X_full.apply(lambda x: le.fit_transform(x))\n","\n","# Split\n","X_train_NB = X_NB[X_NB['type'] ==  1]\n","X_test_NB = X_NB[X_NB['type'] == 0]\n","\n","y_train_NB = le.fit_transform(y_train)\n","y_test_NB = le.fit_transform(y_test)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1646639269667,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"ugbV19aKQnKF"},"outputs":[],"source":["clf_nb = CategoricalNB(min_categories=X_train.nunique())\n","# drop type cols\n","X_train = X_train.drop(['type'], axis = 1)\n","X_test = X_test.drop(['type'], axis = 1)\n","\n","clf_nb.fit(X_train_NB, y_train_NB)\n","\n","naive_scores = cross_validate(clf_nb, X_train_NB, y_train_NB, scoring = ['roc_auc', 'accuracy'])"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1646639271483,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"IWuHJAeeVrhG","outputId":"f06132ff-2dc0-4619-8bd4-4cda8418f9b6"},"outputs":[{"data":{"text/plain":["0.8932793144599438"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["naive_scores['test_roc_auc'].mean()"]},{"cell_type":"markdown","metadata":{"id":"2MPoRmPL3H6M"},"source":["## Model Comparison\n","\n","According to the cross-validation scores in the train data,  the best model is lasso (auc = 0.9), followed  by naive bayes (auc = 8.89), then ridge and the unregularized regression have very similar auc scores (around 0.58).\n","\n","Using the testing data, I show the roc curves  below.\n","The worst models are the un-regularized logistic regression (ULR) and the ridge regression, which is not visible in the graph because the line from the ULR completely overlays the ridge line. This means that the ridge regularization is not working for increasing prediction. This is very surprising. Discussion with Ben, I found that all of his regressions are pretty much similar in terms of prediction. The fact that my results are so different from his could be because in his pipeline he is scaling the X variables or if one of us made a mistake. If is the first, then this shows the importance of scaling the variables: the gain in accuracy is very big. \n","\n","The best regression are lasso and the Naive Bayes classifier. The later is slightly worst, but it does not require parameter tuning to achieve such accuracy. \n","\n","The results form the test roc curves and the cross-validation scores show that the model rankings are maintaned. The best model for prediction in this case is the lasso regression."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":81,"status":"aborted","timestamp":1646638968671,"user":{"displayName":"Rafael Charris Dominguez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00310138805331914177"},"user_tz":480},"id":"NJ90O70BAnk0"},"outputs":[],"source":["roc_plot = metrics.plot_roc_curve(pipe_lasso, X_test, y_test, label = 'Lasso')\n","metrics.plot_roc_curve(pipe_ridge, X_test, y_test, ax = roc_plot.ax_, label = 'Ridge');\n","metrics.plot_roc_curve(pipe_en, X_test, y_test, ax = roc_plot.ax_, label = 'EN');\n","metrics.plot_roc_curve(pipecv, X_test, y_test, ax = roc_plot.ax_, label = 'Logistic Regression');\n","metrics.plot_roc_curve(clf_nb, X_test_NB, y_test_NB, ax = roc_plot.ax_, label = 'NB');\n","sns.lineplot(x = np.linspace(0, 1, 100), y = np.linspace(0, 1, 100))\n","plt.title('ROC curve for all regressions')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPeVFKtposts/KmasuE83J0","collapsed_sections":[],"name":"Charris - hw5.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
