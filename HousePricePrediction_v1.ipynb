{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaxDBHuKHuXy"
      },
      "source": [
        "# <center> CS530 Midterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xttAvtEkHuX1"
      },
      "source": [
        "### Notes:\n",
        "Download two data sets house_SalePrice.csv and house_SalePrice_predict.csv from Canvas and answer the following questions. We want to build a model to predict the sales price of homes. The target variable is 'SalePrice'.\n",
        "\n",
        "The file house_SalePrice.csv is for training.  Each row is represents one house, and contains both features of the house and the sale price.\n",
        "\n",
        "The file house_SalePrice_predict.csv contains additional houses but does not include the sale price. Your goal is to predict the price for these houses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMI57u05HuX2"
      },
      "source": [
        "### Questions\n",
        "\n",
        "#### 1. Pre-processing the data\n",
        "1). There are missing values in the data. Instead of dropping them, fill them in by setting each missing value to the mean/median/mode of the column.  Here are some references if you need them: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html and https://scikit-learn.org/stable/modules/impute.html\n",
        "\n",
        "Note:  If there's a more sophisticated method you prefer, you can use that instead.  Just note it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "Y29KDeCkHuX2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn import linear_model\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "test = pd.read_csv('house_SalePrice_predict.csv')\n",
        "\n",
        "df = pd.read_csv('house_SalePrice.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvoskOUrHuX3"
      },
      "source": [
        "2). use one-hot enconding to convert the categorical variables into dummies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIB06XPvHuX3"
      },
      "source": [
        "#### 2. Regresssion model building\n",
        "In this part, you need to use the data to build a linear model by using OLS first and then build another linear model by using Lasso. Make sure to split the data into training and test sets. Report the performance on test set. Using k-fold cross validation to tune the hyperparameters in Lasso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "72xiq3AiHuX4"
      },
      "outputs": [],
      "source": [
        "X_train , X_test,y_train, y_test = train_test_split(df.drop(['SalePrice'], \n",
        "                                                            axis = 1), df['SalePrice'],\n",
        "                                                     test_size=0.33, random_state=42)\n",
        "\n",
        "col_trans = make_column_transformer((OneHotEncoder(handle_unknown = 'ignore', \n",
        "                                                   drop = 'first'), list(X_train.select_dtypes(include = 'O').columns)),\n",
        "                                    (StandardScaler(), \n",
        "                                     list(X_train.select_dtypes(exclude = 'O').columns)),\n",
        "                                  remainder = 'passthrough')\n",
        "\n",
        "# Use the median to impute missing values \n",
        "transformer = FeatureUnion(\n",
        "    transformer_list=[\n",
        "                      ('features', SimpleImputer(strategy='most_frequent'))])\n",
        "\n",
        "ols_pipe = Pipeline(steps = [\n",
        "                         ('preprocess', col_trans),\n",
        "                         ('transformer', transformer), \n",
        "                         ('model', LinearRegression())\n",
        "                         ])\n",
        "\n",
        "lasso_pipe = Pipeline(steps = [\n",
        "                         ('preprocess', col_trans), \n",
        "                         ('transformer', transformer), \n",
        "                         ('model', linear_model.Lasso(max_iter=10000))])\n",
        "\n",
        "\n",
        "parameters = {'model__alpha': [0.00001, 0.001, 0.01, 1, 10, 100, 1000, 10000, 100000]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OLS SCORING "
      ],
      "metadata": {
        "id": "AqSxP91_8ndh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ols_preds = ols_pipe.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "print(f'The Rsqu for the OLS model is: {round(ols_pipe.score(X_test, y_test), 3)})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7O1LnCHyCV6",
        "outputId": "eed632ed-9c2f-406d-9b80-5e8e8b501d1b"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Rsqu for the OLS model is: 0.73)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LASSO PREDICTIONS"
      ],
      "metadata": {
        "id": "hqrXlPY18qNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best alpha is 100000"
      ],
      "metadata": {
        "id": "6N2IaW36C2Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_lasso = GridSearchCV(lasso_pipe, parameters, cv = 5, scoring = \"r2\")\n",
        "\n",
        "clf_lasso.fit(X_train, y_train)\n",
        "lasso_preds = clf_lasso.predict(X_test)\n",
        "\n",
        "print(f'The Rsqu for the lasso model is: {round(clf_lasso.score(X_test, y_test), 3)})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ict2_jtFp1aS",
        "outputId": "8a6e11e6-0e9e-48d4-b7f8-f99942d5d8ad"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Rsqu for the lasso model is: 0.752)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_lasso.cv_results_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na-1QLAbDuZN",
        "outputId": "5f4970e3-7832-4a46-9202-603dd286920a"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.44046779, 0.10746312, 0.10519147, 0.09429593, 0.09598956,\n",
              "        0.07479467, 0.058988  , 0.06396427, 0.05908289]),\n",
              " 'mean_score_time': array([0.01560607, 0.01334434, 0.01898804, 0.0189887 , 0.01585441,\n",
              "        0.01774817, 0.01626134, 0.01936398, 0.01767683]),\n",
              " 'mean_test_score': array([ 0.73986156,  0.73986193,  0.73986523,  0.74026342,  0.74247091,\n",
              "         0.746031  ,  0.74261025,  0.68084174, -0.00492056]),\n",
              " 'param_model__alpha': masked_array(data=[1e-05, 0.001, 0.01, 1, 10, 100, 1000, 10000, 100000],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'model__alpha': 1e-05},\n",
              "  {'model__alpha': 0.001},\n",
              "  {'model__alpha': 0.01},\n",
              "  {'model__alpha': 1},\n",
              "  {'model__alpha': 10},\n",
              "  {'model__alpha': 100},\n",
              "  {'model__alpha': 1000},\n",
              "  {'model__alpha': 10000},\n",
              "  {'model__alpha': 100000}],\n",
              " 'rank_test_score': array([7, 6, 5, 4, 3, 1, 2, 8, 9], dtype=int32),\n",
              " 'split0_test_score': array([ 0.79446226,  0.79446243,  0.79446392,  0.79461754,  0.79562679,\n",
              "         0.79506811,  0.76221396,  0.64860857, -0.00492844]),\n",
              " 'split1_test_score': array([ 0.55011316,  0.55011308,  0.55011234,  0.55020854,  0.55036323,\n",
              "         0.56056806,  0.60225133,  0.62580171, -0.00384812]),\n",
              " 'split2_test_score': array([ 0.74803711,  0.74803777,  0.74804373,  0.74870808,  0.75372321,\n",
              "         0.76817146,  0.77652575,  0.73340926, -0.01514685]),\n",
              " 'split3_test_score': array([ 8.06104028e-01,  8.06104293e-01,  8.06106703e-01,  8.06362124e-01,\n",
              "         8.08027680e-01,  8.05158207e-01,  7.92243913e-01,  7.10223228e-01,\n",
              "        -6.15621583e-04]),\n",
              " 'split4_test_score': array([ 8.00591263e-01,  8.00592080e-01,  8.00599452e-01,  8.01420798e-01,\n",
              "         8.04613647e-01,  8.01189151e-01,  7.79816292e-01,  6.86165907e-01,\n",
              "        -6.37449930e-05]),\n",
              " 'std_fit_time': array([0.38264122, 0.01355807, 0.01897359, 0.01392481, 0.02975332,\n",
              "        0.02686273, 0.00739345, 0.01032189, 0.01933896]),\n",
              " 'std_score_time': array([0.0017175 , 0.00017003, 0.00724969, 0.00693253, 0.00185976,\n",
              "        0.00600842, 0.00580489, 0.0076323 , 0.00469349]),\n",
              " 'std_test_score': array([0.09708624, 0.09708644, 0.09708825, 0.09721708, 0.09799687,\n",
              "        0.09362669, 0.07082794, 0.03930228, 0.0054378 ])}"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOyitV1aHuX4"
      },
      "source": [
        "#### 3. Classification model building\n",
        "\n",
        "Create a binary variable to indicate whether the sale price is greater than median sale price (=1 if it's higher than median, and 0 otherwise). Build and compare two classification models to predict whether or not the house sells above the median price: a logistic model and a random forest model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Logistic regression "
      ],
      "metadata": {
        "id": "AJaVwBSZ9vgz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "O-ad61ZdHuX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34cc09a-1fa5-4080-a2f5-8efe0c821287"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7729477813923188"
            ]
          },
          "metadata": {},
          "execution_count": 292
        }
      ],
      "source": [
        "df2 = df.copy()\n",
        "df2['above_median'] = df2['SalePrice'].apply(lambda x: 1 if x > np.median(df['SalePrice']) else 0)\n",
        "df2.drop(['SalePrice'], axis = 1, inplace=True)\n",
        "\n",
        "X_train2 , X_test2, y_train2, y_test2 = train_test_split(df2.drop(['above_median'], \n",
        "                                                            axis = 1), df2['above_median'],\n",
        "                                                     test_size=0.33, random_state=42)\n",
        "\n",
        "col_trans = make_column_transformer((OneHotEncoder(handle_unknown = 'ignore', \n",
        "                                                   drop = 'first'), list(X_train2.select_dtypes(include = 'O').columns)),\n",
        "                                    (StandardScaler(), \n",
        "                                     list(X_train2.select_dtypes(exclude = 'O').columns)),\n",
        "                                     remainder = 'passthrough')\n",
        "\n",
        "transformer = FeatureUnion(\n",
        "    transformer_list=[\n",
        "                      ('features', SimpleImputer(strategy='most_frequent'))])\n",
        "\n",
        "\n",
        "logistic_pipe = Pipeline(steps = [\n",
        "                         ('preprocess', col_trans), \n",
        "                         ('transformer', transformer), \n",
        "                         ('model', linear_model.LogisticRegression())])\n",
        "\n",
        "\n",
        "logistic_pipe.fit(X_train2, y_train2)\n",
        "logistic_preds = logistic_pipe.predict(X_test2)\n",
        "\n",
        "\n",
        "matthews_corrcoef(y_test2, logistic_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest "
      ],
      "metadata": {
        "id": "xry5SL7S9zli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "rf_pipe = Pipeline(steps = [\n",
        "                         ('preprocess', col_trans), \n",
        "                         ('transformer', transformer), \n",
        "                         ('model', RandomForestClassifier())])\n",
        "\n",
        "\n",
        "rf_pipe.fit(X_train2, y_train2)\n",
        "rf_preds = rf_pipe.predict(X_test2)\n",
        "\n",
        "\n",
        "matthews_corrcoef(y_test2, rf_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGjxpi1x95YO",
        "outputId": "494ae542-874d-4d3b-a811-63124026ba7c"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7823229278882637"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a lot of variation between test-train splits, but random forest usually performs better than the logistic."
      ],
      "metadata": {
        "id": "z_iaaOZSPszd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51MOqtPpHuX4"
      },
      "source": [
        "#### 4. Model comparasions\n",
        "Now use your OLS and Lasso regressions from part 2 and binarize the output so that they predict 1 if the house is predicted to sell for more than the median, and 0 otherwise.\n",
        "\n",
        "How does the result compare to the Logistic and Random Forest models? Which one is best and why do you think it performs better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "L6QDZLUJHuX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0270dd3-3fa1-473f-aaf8-76dd1e995658"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.725394827191577"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ],
      "source": [
        "ols_preds_binary = ols_preds >= np.median(df['SalePrice'])\n",
        "\n",
        "matthews_corrcoef(y_test2, ols_preds_binary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_preds_binary = lasso_preds >= np.median(df['SalePrice'])\n",
        "\n",
        "matthews_corrcoef(y_test2, lasso_preds_binary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oak-nxR6Ay5F",
        "outputId": "16179caf-883a-45ec-9623-b4aa684eb0d7"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7035039542847492"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show that the logistic and the random forest have very similar results. In some train-test splits the logistic is better, in others the random forest is better. Still both are better than the lasso and the ols predictions. \n",
        "\n",
        "I think this shows a limitation of using methods suited for continuous variables (ols, and lasso) to clasification. They are not very bad, but are sistematically worst than the two classification algorithms. The later take into account by design that the prediction is a categorical variable and use a logistic transformation of a linear function, and a entropy measure to achieve better performance. "
      ],
      "metadata": {
        "id": "gy4TqKyXOcsL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKsGh5WHuX5"
      },
      "source": [
        "#### 5. Test your model (Bonus)\n",
        "\n",
        "Create a csv file with two columns by using house_SalePrice_predict.csv:\n",
        "  1. Your best salePrice prediction.\n",
        "  2. Your best aboveMedian prediction.\n",
        "\n",
        "Name the file as '\\<Your Last Name\\>_prediction.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "j_oAq33AHuX5"
      },
      "outputs": [],
      "source": [
        "lasso_final_preds = clf_lasso.predict(test)\n",
        "\n",
        "logistic_final_preds = logistic_pipe.predict(test)\n",
        "\n",
        "\n",
        "my_predictions = pd.DataFrame({\"salePrice\":lasso_final_preds, \"aboveMedian\": logistic_final_preds})\n",
        "\n",
        "my_predictions.to_csv(\"Charris_prediction.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Charris - CS530_Midterm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
